---
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::html_document2:
      number_sections: yes
bibliography: ../Roads.bib
csl: "../../templates/canadian-journal-of-forest-research.csl" # Insert path for the bib-style

editor_options: 
  chunk_output_type: inline
---

# (APPENDIX) Supplementary Material for Development and assessment of automated forest road projection methods using performance metrics relevant for wildlife {.unnumbered}

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/",
  dpi = 300
)
```

```{r setup2}
library(ggplot2)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(ggpubr)
library(MetBrewer)
library(here)
library(tmap)
library(sf)

devtools::load_all(here())

#set paths
data_path_raw <- "analysis/data/raw_data/"
data_path_drvd <- "analysis/data/derived_data/TSA27"

pal_nm <- "Redon"

fig_widths <- c(min = 1.18, single = 3.543, mid = 5.51, two = 7.48)

```

## Grade penalty and simple cost methods for setting edge weights

The grade penalty method for calculating edge weights is a simplified version of the approach taken by Anderson and Nelson [-@anderson_projecting_2004] that does not distinguish between adverse and favourable grades. We use a modified Digital Elevation Model (DEM) as the input to the edge weight calculation that allows barriers other than slope to be included without incurring additional computation and memory costs. Pixels where roads cannot be built are assigned a value of NA, and negative values indicate costs associated with other features (e.g. stream crossings). Existing roads are assigned a value of 0. All other values in the weight raster are assumed to be elevation and grade is calculated from differences between adjacent locations. Default construction cost values are taken from the BC Interior Appraisal Manual: the base cost of building a road is \$16 178 with an additional cost of \$504 for every 1% grade and a limit of 20% grade. Edges with greater than 20% grade are assigned either NA or an arbitrarily high value. A value of 65 000 is used in our projections to avoid errors when a landing cannot be accessed without crossing an NA cell. Note that grade penalties are not applied adjacent to existing roads, nor do they apply when connecting to a node with a negative value. In the latter case, the edge weight is set to the highest absolute cost. Weights are adjusted for the horizontal distance between nodes to penalize longer diagonal road segments.

```{r echo=TRUE}

gradePenaltyFn <- function(x1, x2, hdistance, baseCost = 16178, limit = 20,
                           penalty = 504, limitWeight = NA){
  #For grade calculation, x1, x2 and hdistance (i.e. horizontal distance between nodes) 
  #must have the same units.
  
  # Don't calculate grade penalty cost if one of the nodes is a barrier.
  cond <- pmin(x1, x2) >= 0
  cond[is.na(cond)] <- F

  # Apply grade penalty if both nodes have elevation values.
  # If one node is a road use base cost.
  grade <- 100 * abs(x1 - x2) * cond / hdistance #This is % (dimensionless)

  slp <- baseCost + grade * penalty * (pmin(x1, x2) > 0) #This is cost per km.
  slp[grade > limit] <- limitWeight

  # If both 0 this is an existing road link. Otherwise it is a barrier.
  slp[!cond] <- abs(pmin(x1, x2))[!cond]

  # Multiply by hdistance to penalize diagonals.
  # Note that multiplying all edge weights by a constant does not change algorithm behaviour,
  # so we ignore the unit of hdistance here.
  slp = slp*hdistance

  return(slp)
}

```

In the simple cost method, we first calculate a slope raster $S$, then calculate an input cost raster $X$ that depends on slope $S$:
\begin{equation}
   X = 
   \begin{cases}
     0 \text{ if existing road} \\
     65000 \text{ if lake}\\
     16178+504S \text{ otherwise}
   \end{cases}
\end{equation}
The weight of an edge between two nodes is the mean value of the input cost raster at each of those nodes, adjusted for the euclidean distance between the nodes: 
```{r echo=TRUE}
simpleCostFn <- function(x1,x2,hdistance){
  # Multiply by hdistance to penalize diagonals.
  # Note that multiplying all edge weights by a constant does not change algorithm behaviour,
  # so we ignore the unit of hdistance here.
  return(hdistance*(x1+x2)/2)
}
```

## Simple cost results

The simple cost method for setting edge weights generally performed worse than the grade penalty and Hardy methods (compare Figures 2 and S\@ref(fig:aspat-perf-fig-simp), and Figures 5 and S\@ref(fig:spat-perf-fig-simp)). 

```{r get-data}
# load csv
meanTable <- read.csv(here(data_path_drvd,"../TSA27", "mean_table.csv"))
matchData <- read.csv(here(data_path_drvd,"../TSA27", "agree_table.csv"))
meanTableRealCuts <- read.csv(here(data_path_drvd,"../TSA27_real_cuts",
                                   "mean_table.csv"))

matchDataRealCuts <- read.csv(here(data_path_drvd,"../TSA27_real_cuts",
                                   "agree_table.csv"))

```

```{r aspat-perf-fig-simp, fig.height=4, fig.width=fig_widths["two"], fig.cap="Proportional difference of projected mean from observed mean road metrics using different simple cost projection method variants within cutblocks, outside cutblocks, and overall (1 ha cost raster). Values greater than zero indicate overprojection and negative values indicate underprojection. Cutblocks not accessible via the observed road network were excluded from analysis. Performance varies among metrics (columns), projection method variants (colours), and within and outside cutblocks."}
# organize data for ggplot
doAspatPlot(meanTableRealCuts)
```

<!-- I have not remade Figure 4 with the old results but we could pull it out of GitHub if we want it.-->

```{r spat-perf-fig-simp, fig.height=4.5, fig.width=fig_widths["two"], fig.cap="Spatial variation of in performance among simple cost projection method variants (1 ha cost raster). F-measure is the harmonic mean of precision and sensitivity, precision is the proportion of predicted pixels that were observed and sensitivity is the proportion of observed pixels that were correctly predicted. Values closer to one indicate better performance. Cutblocks not accessible via the observed road network were excluded from analysis."}
perf_tbl <- doSpatPerf(matchDataRealCuts)
perf_tbl_rng <- perf_tbl %>% group_by(metric) %>% 
  summarise(rng_F = range(F_measure, na.rm = TRUE) %>% round(3) %>% paste0(collapse = " - "), .groups = "drop") %>% pull(rng_F, name = metric)

doSpatPlot(perf_tbl)

```

## All cutblocks results

```{r aspat-perf-fig-all, fig.height=4, fig.width=fig_widths["two"], fig.cap="Proportional difference of projected mean from observed mean road metrics using different projection method variants with a 1 ha resolution weight raster in analysis that includes false cutblocks with no observed access roads. Metrics were summarised within cutblocks, outside cutblocks, and overall. Values greater than 0 indicate overprojection of the metric and negative values indicate an underprojection. "}
meanTable <- read.csv(here(data_path_drvd, "dem", "mean_table.csv"))
doAspatPlot(meanTable)
```

```{r spat-perf-fig-all, fig.height=4.5, fig.width=fig_widths["two"], fig.cap="Comparison of performance metrics for each projection method variant using the 1 ha resolution weight raster, including false cutblocks with no observed access roads. F-measure is the harmonic mean of precision and sensitivity, precision is the proportion of predicted roads that are observed roads and sensitivity is the proportion of observed roads that were correctly predicted. Values closer to one indicate better performance."}
matchData <- read.csv(here(data_path_drvd, "dem", "agree_table.csv"))
perf_tbl2 <- doSpatPerf(matchData)
doSpatPlot(perf_tbl2)

```

## Coarse resolution results

```{r get-data2}
data_path_drvd <- "analysis/data/derived_data/TSA27_real_cuts/"

# load csv
meanTable2 <- read.csv(here(data_path_drvd, "dem_1000", "mean_table.csv")) %>% 
  mutate(resolution = "1000")

matchData2 <- read.csv(here(data_path_drvd, "dem_1000", "agree_table.csv")) %>% 
  mutate(resolution = "1000")

# compare to real cuts
meanTable <- read.csv(here(data_path_drvd, "dem", "mean_table.csv"))
meanTable <- meanTable %>% mutate(resolution = "100") %>% bind_rows(meanTable2) %>% 
  filter((method != "ilcp"| is.na(method)) & (sampleType != "cutOnly" | is.na(sampleType)))

matchData <- read.csv(here(data_path_drvd, "dem", "agree_table.csv"))
matchData <- matchData %>% mutate(resolution = "100") %>% bind_rows(matchData2) %>% 
  filter(method != "ilcp", sampleType != "cutOnly")

```

```{r figA1, fig.height=6, fig.width=fig_widths["two"], fig.cap="Proportional difference of projected mean from observed mean road metrics using different projection methods for the fine resolutiom (1 ha) and coarse resolution (100 ha) weight rasters. Metrics were summarised within cutblocks, outside cutblocks, and overall. Values greater than 0 indicate the projection overestimated the metric and negative values indicate an underestimation."}
# organize data for ggplot
meanTable_long <- meanTable %>% dplyr::select(-runTime, -order) %>% 
  pivot_longer(-c(sampleType, sampleDens, areaMean, resolution, method),
               names_to = "metric", values_to = "response")%>%
  mutate(sampleType = ifelse(sampleType == sampleDens, NA_character_, 
                             sampleType),
         sampleDens = paste(method, sampleType, sampleDens) %>% 
           factor(levels = c("mst NA centroid",
                             "NA NA observed",
                             "mst random low sample density",
                             "mst regular low sample density",
                             "mst random high sample density",
                             "mst regular high sample density",
                             "ilcp regular high sample density",
                             "NA klementQGIS", 
                             "NA NA NA"),
                  labels = c("MST centroid", 
                             "Observed",
                             "MST random low density",
                             "MST regular low density",
                             "MST random high density",
                             "MST regular high density",
                             "ILCP regular high density",
                             "Hardy QGIS",
                             "Cutblocks only")),
         metric = factor(metric,
                         levels = c("roadDensityMean", "roadPresenceMean",
                                    "distanceToRoadMean",
                                    "forestryDisturbanceMean",
                                    "roadDisturbanceMean"),
                         labels = c("Road\ndensity", "Road\npresence",
                                    "Distance\nto road",
                                    "Forestry\ndisturbance\nfootprint",
                                    "Road\ndisturbance\nfootprint")))

observed_values <- filter(meanTable_long, sampleDens == "Observed")
projected_values <- filter(meanTable_long, sampleDens != "Observed")

# get proportional difference from observed
prop_dif <- projected_values %>% 
  left_join(observed_values %>% select(areaMean, metric, resolution, response),
            by = c("areaMean", "metric", "resolution"), 
            suffix = c("_proj", "_obs")) %>% 
  mutate(prop_dif = (response_proj-response_obs)/response_obs,
         areaMean = factor(areaMean, labels = c("Cutblocks", "Outside\ncutblocks",
                                                "Overall")))

prop_dif %>% 
  ggplot(aes(x = sampleDens, prop_dif, fill = resolution))+
  geom_hline(aes(yintercept = 0), color = "grey")+
  geom_col(position = position_dodge2(preserve = "single",
                                      width = 0.75))+
  facet_grid(areaMean ~ metric, scales = "free")+
  theme_classic()+
  scale_fill_manual(values = met.brewer(pal_nm), labels = c("1 ha", "100 ha"))+
  theme(text = element_text(size = 10), axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "right",
        axis.text.x=element_text(size=11, angle=50, vjust=1, hjust=1))+
  coord_flip()+
  labs(fill = "Resolution")

```

```{r figA2, fig.height=4.5, fig.width=fig_widths["two"], fig.cap="Comparison of performance metrics for each projection method variant for the fine resolutiom (1 ha) and coarse resolution (100 ha) weight rasters. F measure is the harmonic mean of precision and sensitivity, precision is the proportion of predicted roads that are observed roads and sensitivity is the proportion of observed roads that were correctly predicted."}

matchData_sum <- matchData %>% ungroup() %>%
  mutate(agreement = factor(agreement,
                            levels = c("False negative", "False positive",
                                       "Agree roaded","Agree roadless",
                                       "Pre-existing roads")),
         sampleDens = paste(method, sampleType, sampleDens) %>%
           str_replace("1e-06", "low density") %>%
           str_replace("1e-05", "high density") %>%
           str_replace("centroid low density", "centroid") %>%
           str_replace("klementQGIS NA", "klementQGIS") %>%
           factor(levels = c("mst centroid",
                             "mst random low density",
                             "mst regular low density",
                             "mst random high density",
                             "mst regular high density",
                             "ilcp regular high density",
                             "NA klementQGIS"),
                  labels = c("MST centroid", 
                             "Random low density",
                             "Regular low density",
                             "Random high density",
                             "Regular high density",
                             "ILCP regular high density",
                             "Hardy QGIS")),
         metric = factor(metric,
                         levels = c("roadPresence",
                                    "forestryDisturbance",
                                    "roadDisturbance"),
                         labels = c("Road presence",
                                    "Forestry disturbance\nfootprint",
                                    "Road disturbance\nfootprint"))) %>%
  group_by(metric, sampleDens, agreement, resolution) %>%
  summarise(count = sum(count), .groups = "drop_last") %>%
  mutate(perc = count/sum(count)*100)

# performance table
perf_tbl <- matchData_sum %>% 
  pivot_wider(c(metric, sampleDens, resolution), names_from = agreement, 
              values_from = count) %>% 
  mutate(sensitivity = `Agree roaded`/(`Agree roaded` + `False negative`),
         precision = `Agree roaded`/(`Agree roaded` + `False positive`),
         F_measure = (2*precision*sensitivity)/(precision+sensitivity)) %>% 
  select(resolution, metric, sampleDens, sensitivity, precision, F_measure) 

perf_tbl_rng <- perf_tbl %>% group_by(metric, resolution) %>% 
  summarise(rng_F = range(F_measure) %>% round(3) %>% paste0(collapse = " - "), .groups = "drop") %>% pull(rng_F, name = metric)

perf_tbl %>% 
  pivot_longer(c(sensitivity, precision, F_measure), names_to = "perf_meas",
               values_to = "value") %>% 
  mutate(perf_meas = factor(perf_meas,
                            labels = c("F measure", "Precision", "Sensitivity"))) %>% 
  ggplot(aes(sampleDens, value, fill = resolution))+
  geom_col(position = position_dodge2(preserve = "single",
                                      width = 0.75))+
  scale_fill_manual(values = met.brewer(pal_nm), labels = c("1 ha", "100 ha"))+
  facet_grid(metric ~ perf_meas)+
  coord_flip()+
  scale_y_continuous(limits = c(0, 1), breaks = 0:5/5)+
  theme_classic()+
  labs(y = "Performance", x = NULL, fill = "Resolution")

```

## Benchmarking road projection methods

We compared the speed and peak RAM usage of the MST and ILCP methods with landings regularly sampled at 10 per km^2^ (high density) or using only the cutblock centroids as landings on weight rasters of three different resolutions (1000 m, 500 m and 100 m), giving 18 297, 72 633 and 1 815 825 nodes in the cost graph, respectively. Each cell in the raster becomes a node in the graph used to determine the shortest path which increases the time and memory requirements of the projection. Benchmarking was done using Azure Standard A4 v2 nodes which have 4 cores and 8 GB of RAM, chosen to match a typical laptop. Each projection was run on a separate node and the `peakRAM` package was used to measure the total time to run the projection and the maximum memory used at any point in the analysis [@quinn_peakram_2024]. We also timed the execution of the Hardy QGIS plugin with the 100 m resolution weights raster.

Execution time and memory requirements increase with the number of nodes in the graph, and the slope of that increase is highest for the MST method (Figure S\@ref(fig:speed-fig)). Except on the very small landscape the ILCP method is faster than the MST method. Reducing the number of landings by targeting centroids also reduces execution time (Figure S\@ref(fig:speed-fig)). The Hardy method is significantly faster than any of the R package methods.

```{r speed-fig, fig.width=fig_widths["two"], fig.cap="Speed and peak memory usage of the ILCP and MST road projection methods with landings regularly sampled at 10 per km^2^ (high density) or using cutblock centroids as landings, with weight rasters of three different resolutions (1000 m, 500 m and 100 m) giving 18 297, 72 633 and 1 815 825 nodes in the cost graph, respectively. The time to run the Hardy QGIS plugin with the 100 m raster is also shown. Both number of nodes and time are displayed on log transformed axes."}
# Get run times from bench marking results
# compile results after running on cloud
bench_res <- list.files(here::here("analysis/data/derived_data/bench_results"),
                        pattern = "bench_.*.rds",
                        full.names = TRUE) %>%
  purrr::map(readRDS) %>%
  bind_rows() %>%
  tidyr::separate(id, into = c("method", "sampleType", "sampleDens", "agg", "cutblocks_real"),
                  sep = "_",
                  extra = "merge", convert = TRUE)

ch_time <- structure(list(id = 1, resolution = 50, n_verticies = 21791850, 
    n_edges = 87138977, Elapsed_Time_sec = 10001.606, Total_RAM_Used_MiB = 4018.7, 
    Peak_RAM_Used_MiB = 10752.7), class = "data.frame", row.names = c(NA, 
-1L))

# Hardy timing added below for 100m
# To run Hardy alg: 2mins 10s
# Plus to Union with existing roads 2 mins 34s
# Hardy with 10m resolution 14097.66 seconds
hardy_time <- 2*60+34

ilcp_time <- bench_res %>% 
  filter(method == "ilcp", cutblocks_real == "revelstoke_real",
         resolution == 100) %>% 
  pull(Elapsed_Time_sec) %>% units::set_units("sec") %>%
  units::set_units("hours") %>% round(2) %>% units::drop_units()

mst_time <- bench_res %>% 
  filter(method == "mst", cutblocks_real == "revelstoke_real",
         resolution == 100, sampleDens == 1e-05, sampleType == "regular") %>% 
  pull(Elapsed_Time_sec) %>% units::set_units("sec") %>%
  units::set_units("hours") %>% round(2) %>% units::drop_units()

# Formula for extrapolating RAM needed for road projection
# bench_res %>% 
#   filter(method == "mst", sampleType == "regular", sampleDens == 1e-05,
#          cutblocks_real== "revelstoke_real") %>% 
#   arrange(desc(resolution)) %>% 
#   {lm(log10(Peak_RAM_Used_MiB)~log10(n_verticies), data = .)}
# 10^(-1.6+0.769*log10(21791850))

minor_breaks <- rep(1:4*2, 21)*(10^rep(-10:10, each=9))
bench_res %>%
  add_row(cutblocks_real = "revelstoke_real", sampleType = "regular", sampleDens = 1.5e-05, 
          method = "Hardy QGIS", Elapsed_Time_sec = hardy_time, n_verticies = 1815825) %>% 
  # add_row(cutblocks_real = "revelstoke_real", sampleType = "centroid", sampleDens = 1.5e-05, 
  #         method = "ilcp", Elapsed_Time_sec = ch_time$Elapsed_Time_sec, 
  #         n_verticies = ch_time$n_verticies, Peak_RAM_Used_MiB = ch_time$Peak_RAM_Used_MiB) %>% 
  filter(cutblocks_real == "revelstoke_real", sampleType %in% c("regular", "centroid"), 
         sampleDens > 1e-06 | sampleType == "centroid") %>% 
  mutate(Peak_RAM_Used_MiB = Peak_RAM_Used_MiB * 0.001048576, 
         sampleDens = ifelse(sampleType == "centroid", 0.1, sampleDens * 1000^2)) %>% 
  pivot_longer(c(Elapsed_Time_sec, Peak_RAM_Used_MiB), names_to = "variable", 
               values_to = "values") %>% 
  ggplot(aes(y = values, col = interaction(method, sampleDens), x = n_verticies,
             group = interaction(method, sampleDens), 
             shape = interaction(method, sampleDens)))+
  geom_line(linewidth = 1.05)+
  geom_point(size = 2)+
  scale_colour_manual(labels = c("ILCP centroid", "MST centroid",
                                 "ILCP high density", "MST high density", "Hardy QGIS"),
                      values = c("#ab84a5","#732f30", "#df8d71", "#1e5a46", "#af4f2f"))+
  scale_shape_discrete(solid = FALSE, 
                       labels = c("ILCP centroid", "MST centroid", 
                                  "ILCP high density", "MST high density", "Hardy QGIS"))+
  scale_y_log10(minor_breaks = minor_breaks)+
  scale_x_log10(minor_breaks = minor_breaks, limits = c(1e04, NA))+
  facet_wrap(~variable, nrow = 2, scales = "free", 
             labeller = labeller(variable = c(Elapsed_Time_sec = "Time (s)", 
                                              Peak_RAM_Used_MiB = "Peak RAM used (GB)"))) +
  labs(y = NULL, x = "# Nodes", col = "Method", shape = "Method")+
  theme_bw()
```

## Profiling

Profiling with the `Rprof` function shows which parts of a function take the longest. On a test run with 1 million cells in the weight raster and 10 landings we profiled the performance of the ILCP and MST methods. Both methods begin by constructing a full graph, which took 3-5 s. For the ILCP method, iteratively determining the shortest paths is the only other step that takes more than a second to run, and this run time will increase with the number of landings (Figure S\@ref(fig:profiling)). For the MST method, calculating cost distances among landings and from each landing to the nearest point on the existing road takes the most time, which also increases with the number of landings (Figure S\@ref(fig:profiling)). Determining the locations of the paths in the minimum spanning tree also takes about 6 seconds. We experimented with using graph construction methods in the package `gdistance` [@van_etten_r_2017], but Lochhead et al's [-@lochhead_demo_2018; -@lochhead_linking_2022] method is faster (Figure S\@ref(fig:profiling)).     

```{r profiling, fig.cap="Profiling results for the ILCP, MST, and ILCP using the gdistance package methods, showing functions that take more than one second to run on a cost raster with 1 million cells and 10 landings. Functions higher in the call stack are called by functions lower down.", fig.height=6, fig.width=fig_widths["two"]}

ilcp_100 <- profstat(here::here("analysis/data/derived_data/ilcp_100_Rprof.out"))
mst_100 <- profstat(here::here("analysis/data/derived_data/mst_100_Rprof.out"))
ilcp_gd_100 <- profstat(here::here("analysis/data/derived_data/ilcp_gdistance_100_Rprof.out"))

bind_rows(ILCP = ilcp_100, MST = mst_100, `ILCP gdistance` = ilcp_gd_100, .id = "method") %>% 
  mutate(label = case_when(label == ".External2" ~ "", 
                           label == "lazy_eval" ~ "", 
                           label == "gdistance::geoCorrection" & depth == 8 ~ "", 
                           label == "graph.adjacency.sparse" ~ "", 
                           TRUE ~ label)) %>% 
  mutate(method = factor(method, levels = c("ILCP", "MST", "ILCP gdistance"))) %>% 
  ggplot(aes(y = depth, label = label))+
  geom_linerange(aes(xmin = st_time, xmax = end_time-0.1))+
  geom_text(aes(y = depth+0.35, x = st_time), hjust = 0, 
            size.unit = "pt", size = 10)+
  scale_x_continuous(expand = expansion(mult = c(0, 0.15)))+
  labs(y = "Call stack", x = "Time (s)")+
  theme_bw()+
  theme(panel.grid = element_blank())+
  facet_grid(method~.)
# interactive version
# profvis::profvis(prof_input = here::here("analysis/data/derived_data/ilcp_100_Rprof.out"))
```

## Fort Nelson map

```{r ftNelson, cache=TRUE, fig.height=6, fig.width=fig_widths["two"], fig.cap="Existing roads in Fort Nelson. Cost based road projection methods are not expected to be useful in areas where roads are built for purposes other than accessing a target location at minimal cost. For example, in the Northeast corner of the Fort Nelson Timber Supply Area seismic lines show a different pattern than foresty roads."}
ftN_roads_obs <- read_sf(here("analysis/data/derived_data/combined_ft_nelson_roads.gpkg"))

# only show top right corner
bb <- st_bbox(ftN_roads_obs) 

bb[["xmin"]] <- bb[["xmax"]] - (bb[["xmax"]] - bb[["xmin"]])/2
bb[["ymin"]] <- bb[["ymax"]] - (bb[["ymax"]] - bb[["ymin"]])/2

ftN_roads_obs_NE <- st_crop(ftN_roads_obs, bb)

tm_shape(ftN_roads_obs_NE, is.master = TRUE)+
  tm_lines()
```

\newpage

# Literature Cited {.unnumbered}

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

# Colophon {.unnumbered}

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r colophon, cache = FALSE}
# which R packages and versions?
#if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())
```
